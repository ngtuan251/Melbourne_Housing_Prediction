{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# __Melbourne Housing Prediction - Machine Learning Project__\n---\n","metadata":{"id":"NcErhpkZvUXN"}},{"cell_type":"markdown","source":"## About Our Group \n\n- BS20DSY031 - Tuan Nguyen\n- BS20DSY029 - Thanh Nguyen\n- BS20DSY033 - Minh Le\n- BS20DSY041 - Hai Nguyen","metadata":{"id":"hq0uGcWJvebv"}},{"cell_type":"markdown","source":"# 1. Introduction¶\n\n*   The aim of this notebook is to build useful models that can predict Melbourne (located in Victoria) housing prices based on a set of scrapped features made available in the Melbourne Housing Dataset. Also, it provides practice insights for house buyers into the Melbourne Housing Market.\n \n\n*   The dataset is very interesting to explore since Melbourne is one of the world's most liveable cities. It's also interesting to explore the Plotly library capability & create interactive choropleth maps, similar to the notebook I wrote about Australian Geographic Data Plots.\n\n","metadata":{"id":"6p5CZUTcyulF"}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly\nfrom plotly.subplots import make_subplots\nfrom sklearn import preprocessing\nimport seaborn as sns\nfrom scipy import stats\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom sklearn.base import BaseEstimator,TransformerMixin\n\n%matplotlib inline\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', 50)\n\nline_colors = [\"#7CEA9C\", '#50B2C0', \"rgb(114, 78, 145)\", \"hsv(348, 66%, 90%)\", \"hsl(45, 93%, 58%)\"]","metadata":{"id":"502YKua0t9ZB","outputId":"0872df90-6914-4931-96a7-86296c962dc9","execution":{"iopub.status.busy":"2022-06-17T06:20:45.105257Z","iopub.execute_input":"2022-06-17T06:20:45.105872Z","iopub.status.idle":"2022-06-17T06:20:50.097970Z","shell.execute_reply.started":"2022-06-17T06:20:45.105783Z","shell.execute_reply":"2022-06-17T06:20:50.097061Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# 2. Melbourne Housing Dataset\n\n## 2.1. Data Preparation\n\nLet's review some of the features that are available in the **Melbourne Housing Dataset**.\n\n- Suburb: Suburb\n\n- Address: Address\n\n- Rooms: Number of rooms\n\n- Price: Price in Australian dollars\n\n- Method:  \nS - property sold;  \nSP - property sold prior;  \nPI - property passed in;  \nPN - sold prior not disclosed;  \nSN - sold not disclosed;  \nNB - no bid;  \nVB - vendor bid;  \nW - withdrawn prior to auction;  \nSA - sold after auction;  \nSS - sold after auction price not disclosed.  \nN/A - price or highest bid not available.  \n\n- Type:  \n    br - bedroom(s);  \n    h - house,cottage,villa, semi,terrace;  \n    u - unit, duplex;  \n    t - townhouse;  \n    dev site - development site;  \n    o res - other residential.  \n\n- SellerG: Real Estate Agent\n\n- Date: Date sold\n\n- Distance: Distance from CBD in Kilometres\n\n- Regionname: General Region (West, North West, North, North east …etc)\n\n- Propertycount: Number of properties that exist in the suburb.\n\n- Bedroom2 : Scraped # of Bedrooms (from different source)\n\n- Bathroom: Number of Bathrooms\n\n- Car: Number of carspots\n\n- Landsize: Land Size in Metres\n\n- BuildingArea: Building Size in Metres\n\n- YearBuilt: Year the house was built\n\n- CouncilArea: Governing council for the area\n\n- Lattitude: Self explanitory\n\n- Longtitude: Self explanitory\n\n","metadata":{"id":"AO5P7ihVrx4J"}},{"cell_type":"code","source":"df = pd.read_csv('https://raw.githubusercontent.com/monilshah98/Melbourne-House-Price-Prediction/master/DataSet/Melbourne_Housing_Dataset.csv')","metadata":{"id":"xIi24iY9j_tx","execution":{"iopub.status.busy":"2022-06-17T06:22:48.488972Z","iopub.execute_input":"2022-06-17T06:22:48.489345Z","iopub.status.idle":"2022-06-17T06:22:49.145110Z","shell.execute_reply.started":"2022-06-17T06:22:48.489314Z","shell.execute_reply":"2022-06-17T06:22:49.144298Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"id":"WmkZZiyCkUWm","outputId":"68ee2c04-4b90-43b0-a00b-a8c6e1e2c42a","execution":{"iopub.status.busy":"2022-06-17T06:22:51.041775Z","iopub.execute_input":"2022-06-17T06:22:51.042362Z","iopub.status.idle":"2022-06-17T06:22:51.078226Z","shell.execute_reply.started":"2022-06-17T06:22:51.042327Z","shell.execute_reply":"2022-06-17T06:22:51.077384Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"id":"G8GUGOCOkVBH","outputId":"45184707-922a-41b4-ee61-8faaa9f7e388","execution":{"iopub.status.busy":"2022-06-17T06:22:51.975341Z","iopub.execute_input":"2022-06-17T06:22:51.976051Z","iopub.status.idle":"2022-06-17T06:22:52.028546Z","shell.execute_reply.started":"2022-06-17T06:22:51.976013Z","shell.execute_reply":"2022-06-17T06:22:52.027708Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"> We have total 34854 housing transactional records. However, there are significant numbers of null values, we will do some data cleaning first before conducting Exploratory Data Analysis.","metadata":{"id":"qH3Uaafwz10v"}},{"cell_type":"markdown","source":"There are many rows with missing values of the\ntarget variable (**Price**).  \n\nSince the imputation of these values could increase bias in input data, we should remove all the records whose NaN values in **Price**. ","metadata":{"id":"5HXkJ_9Zwb9Y"}},{"cell_type":"code","source":"df.dropna(subset=['Price'], how='all', inplace=True)","metadata":{"id":"SkrHXfF7wdG3","execution":{"iopub.status.busy":"2022-06-17T06:22:56.338945Z","iopub.execute_input":"2022-06-17T06:22:56.339295Z","iopub.status.idle":"2022-06-17T06:22:56.353219Z","shell.execute_reply.started":"2022-06-17T06:22:56.339266Z","shell.execute_reply":"2022-06-17T06:22:56.352206Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Some Data Cleaning \ndf.drop_duplicates(subset=['Address'],inplace=True) # Some addresses actually have multiple entries\ndf.rename ({'Bedroom2': 'Bedrooms'}, axis = 1, inplace = True)\n# df.index = df['Address'] # set dataframe index, since it's not really a useful feature \n# del df['Address'] # let's also delete the column","metadata":{"id":"J0SxR3yllW_k","execution":{"iopub.status.busy":"2022-06-17T06:22:57.005980Z","iopub.execute_input":"2022-06-17T06:22:57.006339Z","iopub.status.idle":"2022-06-17T06:22:57.025220Z","shell.execute_reply.started":"2022-06-17T06:22:57.006310Z","shell.execute_reply":"2022-06-17T06:22:57.024491Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"> Columns with more than 55% values missing should be removed\nfrom the original dataset since it is difficult to impute these\nmissing values with an acceptable level of accuracy. ","metadata":{"id":"oH3GmpFztUJR"}},{"cell_type":"code","source":"missing_percentages = df.isnull().sum(axis = 0) / len(df) \nprint(missing_percentages)\ndf = df.loc[:, (missing_percentages < 0.55)]","metadata":{"id":"roVXA7wUtYzA","outputId":"322102ff-3b8c-4506-ddd7-d95df9f1700c","execution":{"iopub.status.busy":"2022-06-17T06:22:59.121943Z","iopub.execute_input":"2022-06-17T06:22:59.122287Z","iopub.status.idle":"2022-06-17T06:22:59.155636Z","shell.execute_reply.started":"2022-06-17T06:22:59.122257Z","shell.execute_reply":"2022-06-17T06:22:59.154819Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Next, we will create a helper class which will be used to visualize missing data and feature importances.","metadata":{"id":"SEqRu_pT1FSt"}},{"cell_type":"code","source":"# !pip install shap\n# !pip install catboost\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport shap\nfrom catboost import CatBoostClassifier,CatBoostRegressor\nfrom sklearn.feature_selection import SelectKBest,f_regression\nfrom xgboost import plot_importance,XGBClassifier,XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import preprocessing\n\n\n#Notebook Helper Class\nclass transformer(BaseEstimator, TransformerMixin):\n    def __init__(self, drop_nan=False, show_nan=False, select_dtype=False, title='Title', figsize=(None,None), feature_importance = False, target = 'Price'):\n        self.drop_nan = drop_nan\n        self.show_nan = show_nan\n        self.select_dtype = select_dtype\n        self.title = title\n        self.feature_importance = feature_importance\n        self.figsize = figsize\n        self.target = target\n\n\n  # Apply Some Transformation to the Feature Matrix\n    def transform(self, X):\n\n    \n    #'''show NaN % in DataFrame'''\n        if(self.show_nan):\n        \n            fig, ax = plt.subplots(figsize = self.figsize)\n            nan_val = (X.isnull().sum()/len(X)*100).sort_values(ascending = False)\n            cmap = sns.color_palette(\"plasma\")\n            for i in ['top', 'right', 'bottom', 'left']:\n                ax.spines[i].set_color('black')\n                ax.spines['top'].set_visible(True);ax.spines['right'].set_visible(False)\n                ax.spines['bottom'].set_visible(False);ax.spines['left'].set_visible(False)\n                sns.barplot(x=nan_val,y=nan_val.index, edgecolor='k',palette = 'rainbow')\n                plt.title(self.title);ax.grid(ls='--',alpha = 0.9);plt.show()\n                return\n\n\n    #''' Drop All NaN values in DataFrame'''\n        if(self.drop_nan):\n            X = X.dropna();\n            return X\n\n        if self.select_dtype:\n            num = X.loc[:,X.dtypes != 'object']\n            cat = X.loc[:,X.dtypes == 'object']\n            return num, cat\n\n        if self.feature_importance:\n\n        # Plot Correlation to Target Variable only\n            def corrMat2(df,target=self.target,figsize=(9,0.5),ret_id=False):\n\n                corr_mat = df.corr().round(2);shape = corr_mat.shape[0]\n                corr_mat = corr_mat.transpose()\n                corr = corr_mat.loc[:, df.columns == self.target].transpose().copy()\n\n                if(ret_id is False):\n                    f, ax = plt.subplots(figsize=figsize)\n                    sns.heatmap(corr,vmin=-0.3,vmax=0.3,center=0, \n                          cmap=cmap,square=False,lw=2,annot=True,cbar=False)\n                    plt.title(f'Feature Correlation to {self.target}')\n\n                if(ret_id):\n                    return corr\n\n            def feature_importance(df, feature=self.target, n_est=500):\n                num_df0,_ = transformer(select_dtype=True).transform(X=df)\n                num_df = transformer(drop_nan=True).transform(X=num_df0)\n      \n        #  Input dataframe contains numeric features and target feature\n                X = num_df.copy()\n                y = num_df[feature].copy()\n                del X[feature]\n\n        #  CORRELATION\n                imp = corrMat2(num_df,feature,figsize=(15,0.5),ret_id=True)\n                del imp[feature]\n                s1 = imp.squeeze(axis=0);s1 = abs(s1)\n                s1.name = 'Correlation'\n\n        #   SHAP\n                model = CatBoostRegressor(silent=True,n_estimators=n_est).fit(X,y)\n                explainer = shap.TreeExplainer(model)\n                shap_values = explainer.shap_values(X)\n                shap_sum = np.abs(shap_values).mean(axis=0)\n                s2 = pd.Series(shap_sum,index=X.columns,name='Cat_SHAP').T\n\n        #   RANDOMFOREST\n                model = RandomForestRegressor(n_est,random_state=0, n_jobs=-1)\n                fit = model.fit(X,y)\n                rf_fi = pd.DataFrame(model.feature_importances_,index=X.columns,\n                            columns=['RandForest']).sort_values('RandForest',ascending=False)\n                s3 = rf_fi.T.squeeze(axis=0)\n\n        #   XGB \n                model=XGBRegressor(n_estimators=n_est,learning_rate=0.5,verbosity = 0)\n                model.fit(X,y)\n                data = model.feature_importances_\n                s4 = pd.Series(data,index=X.columns,name='XGB').T\n\n        #   KBEST\n                model = SelectKBest(k=X.shape[1], score_func=f_regression)\n                fit = model.fit(X,y)\n                data = fit.scores_\n                s5 = pd.Series(data,index=X.columns,name='K_best')\n\n        # Combine Scores\n                df0 = pd.concat([s1,s2,s3,s4,s5],axis=1)\n                df0.rename(columns={'target':'lin corr'})\n\n                x = df0.values \n                min_max_scaler = preprocessing.MinMaxScaler()\n                x_scaled = min_max_scaler.fit_transform(x)\n                df = pd.DataFrame(x_scaled,index=df0.index,columns=df0.columns)\n                df = df.rename_axis('Feature Importance via', axis=1)\n                df = df.rename_axis('Feature', axis=0)\n                df['total'] = df.sum(axis=1)\n                df = df.sort_values(by='total',ascending=True)\n                del df['total']\n                fig = px.bar(df,orientation='h',barmode='stack',color_discrete_sequence=line_colors)\n                fig.update_layout(template='plotly_white',height=self.figsize[1],width=self.figsize[0],margin={\"r\":0,\"t\":60,\"l\":0,\"b\":0});\n                for data in fig.data:\n                    data[\"width\"] = 0.6 #Change this value for bar widths\n                fig.show()\n        \n        feature_importance(X)","metadata":{"id":"qYTWTGiKqlYj","execution":{"iopub.status.busy":"2022-06-17T06:23:11.438248Z","iopub.execute_input":"2022-06-17T06:23:11.438622Z","iopub.status.idle":"2022-06-17T06:23:12.922136Z","shell.execute_reply.started":"2022-06-17T06:23:11.438589Z","shell.execute_reply":"2022-06-17T06:23:12.921307Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# 2.2. Categorical & Ordinal Features\n- We have a few categorical features which can be handy for EDA, as well as for model features, such as One-Hot Encoding/ GetDummies.\nSold_Month & Sold_Year can be extracted from Date.\n\n- SUBURB probably doesn't tell us any more than the POSTCODE does, but useful for EDA.\n","metadata":{"id":"Ti0ZNxbkpz4O"}},{"cell_type":"code","source":"#Divide features into categorical and ordinal\ndf.Postcode = df.Postcode.astype('object') #Convert Postcode in to object data type\ndf_num, df_cat = transformer(select_dtype=True).transform(X=df.copy())\n\n\n#Create Sold_Month & Sold_Year\ndf_num[['Sold_Month', 'Sold_Year']] = df_cat['Date'].str.split('/', 2, expand=True).loc[:,1:2].astype('float64')\ndf_cat.drop(['Date'], inplace=True, axis=1)\n\ndf_EDA = pd.concat([df_cat, df_num], axis=1)","metadata":{"id":"EYMH3kKMuGWF","execution":{"iopub.status.busy":"2022-06-17T06:23:16.300954Z","iopub.execute_input":"2022-06-17T06:23:16.301615Z","iopub.status.idle":"2022-06-17T06:23:16.381735Z","shell.execute_reply.started":"2022-06-17T06:23:16.301580Z","shell.execute_reply":"2022-06-17T06:23:16.380907Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# 3. UNIVARIATE DATA ANALYSIS\n\n## 3.1. Data Distributions Histograms\n","metadata":{"id":"PQ1L771DzkkV"}},{"cell_type":"code","source":"import plotly.offline as pyo\npyo.init_notebook_mode()","metadata":{"id":"57tb0xcrKu5Q","outputId":"daf7352e-ffb6-44d9-96b6-684b7c5ccb2d","execution":{"iopub.status.busy":"2022-06-17T06:23:20.152813Z","iopub.execute_input":"2022-06-17T06:23:20.153174Z","iopub.status.idle":"2022-06-17T06:23:20.227260Z","shell.execute_reply.started":"2022-06-17T06:23:20.153143Z","shell.execute_reply":"2022-06-17T06:23:20.226323Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\n# Plot Histogram, Boxplot using Plotly\ndef px_stats(df, n_cols=4, to_plot=None, height=800, w=None):\n    df_num, df_cat = transformer(select_dtype=True).transform(df)\n    numeric_cols = df_num.columns\n    n_rows = -(-len(numeric_cols) // n_cols)\n    row_pos, col_pos = 1, 0\n    fig = make_subplots(rows=n_rows, cols=n_cols,subplot_titles=numeric_cols.to_list())\n\n    for col in numeric_cols:\n        if (to_plot == 'histogram'):\n            trace = go.Histogram(x=df_num[col],showlegend=False,autobinx=True,\n                                marker = dict(color = 'rgb(27, 79, 114)',\n                                line=dict(color='white',width=0)))\n        else:\n            trace = getattr(px, to_plot)(df_num[col],x=df_num[col])[\"data\"][0]\n          \n        if col_pos == n_cols: \n            row_pos += 1\n        col_pos = col_pos + 1 if (col_pos < n_cols) else 1\n        fig.add_trace(trace, row=row_pos, col=col_pos)\n\n    fig.update_traces(marker = dict(color = 'rgb(27, 79, 114)',\n                    line=dict(color='white',width=0)))\n    fig.update_layout(template='plotly_white');fig.update_layout(margin={\"r\":0,\"t\":60,\"l\":0,\"b\":0})\n    fig.update_layout(height=height,width=w)\n    fig.show()","metadata":{"id":"O5FwRGf51pce","execution":{"iopub.status.busy":"2022-06-17T06:23:22.652899Z","iopub.execute_input":"2022-06-17T06:23:22.653261Z","iopub.status.idle":"2022-06-17T06:23:22.664211Z","shell.execute_reply.started":"2022-06-17T06:23:22.653231Z","shell.execute_reply":"2022-06-17T06:23:22.663281Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"- The most common price range of a property; 500k-1000k AUD, which makes up about 50% of all properties.\n- We can note 1-bedroom properties are quite uncommon in Melbourne, the most common being a 3-bedroom property, typically having 1 or 2 bathrooms & garage with 1 or 2 car slots.\n- The last months of a year like September, October, November are the months with the highest sales activity and then it decreases until hitting the lowest point in next January.\n- Some of the properties have a very large number of garages slots so it would make sense to just remove them but lets just keep them anyway. Quite a large number of features have *skewed distributions*. Let's note to do some cleaning.","metadata":{"id":"e04Cv3Uou8-H"}},{"cell_type":"code","source":"px_stats(df_EDA, to_plot='histogram') # interactive","metadata":{"id":"B0s7213u0-ot","outputId":"5d5f5816-af72-464f-dee5-a03163b754c4","execution":{"iopub.status.busy":"2022-06-17T06:23:26.671504Z","iopub.execute_input":"2022-06-17T06:23:26.672358Z","iopub.status.idle":"2022-06-17T06:23:27.700441Z","shell.execute_reply.started":"2022-06-17T06:23:26.672303Z","shell.execute_reply":"2022-06-17T06:23:27.699419Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## 3.2. Data Distributions Boxplots\n- Complementary to histograms, boxplots, indicate outliers a little more clearly, as well as useful statistics about min, max, median & q1/q3 values.\n\n- We will lean towards using tree based methods; It is often stated that, ensemble approaches such as *Random Forest* are not sensitive to outliers, however there are counter arguments that state the complete opposite as shown on stackexchange.\n\n- That said our data contains quite a lot of outliers, which is to be expected from a non consistent selling standard/rules for properties, allowing certain properties to be prices above/below values of similar properties depending on specific circumstaces.\n\n- It is interesting to look into creating models for a specific subset of our data ( eg. similar suburbs, low cost suburbs, presold properties and so on ), in an attempt to get around these outliers. One model for the entire dataset seems like a huge stretch, and most definitely will have accuracy limits.","metadata":{"id":"k46ojLGq0tR9"}},{"cell_type":"code","source":"px_stats(df_EDA, to_plot='box',height=550)","metadata":{"id":"zZGeG4oo6yGp","outputId":"265235ca-c6fb-4c01-eee2-a06617e7ff71","execution":{"iopub.status.busy":"2022-06-17T06:23:27.704068Z","iopub.execute_input":"2022-06-17T06:23:27.704652Z","iopub.status.idle":"2022-06-17T06:23:30.067262Z","shell.execute_reply.started":"2022-06-17T06:23:27.704611Z","shell.execute_reply":"2022-06-17T06:23:30.066318Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"\n## 3.3. Target Model Feature Importance Evaluation\n\n- We can use multiple approaches, even early on, to quickly evaluate which features have most weight in a model evaluation to get a better understanding of not only their imporance but also how different models use these features in their evaluation. \n\n- It is easy to understand that **Distance** (to CBD), **Number of rooms**, **Number of Bathrooms**, **Number of Bedrooms** are the most important features.\n\n- Although there are a number of features that have little to no impact, the only irrelevant feature seems to be **Sold_Month**, **Sold_Year**, **Property_count** which we ought to drop, a little later.\n\n\n","metadata":{"id":"EPPGuo8s0xPM"}},{"cell_type":"code","source":"transformer(feature_importance=True,figsize=(800,400),target='Price').transform(X=df_EDA)","metadata":{"id":"7XckxCV1dmiO","outputId":"1e2fcccf-544b-4dae-acb3-fd761746bbfc","execution":{"iopub.status.busy":"2022-06-17T06:23:36.822665Z","iopub.execute_input":"2022-06-17T06:23:36.823069Z","iopub.status.idle":"2022-06-17T06:24:24.896873Z","shell.execute_reply.started":"2022-06-17T06:23:36.823037Z","shell.execute_reply":"2022-06-17T06:24:24.896066Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df_EDA.drop(columns = ['Propertycount', 'Sold_Month', 'Sold_Year'], inplace = True)","metadata":{"id":"ryKCEZtRBjx7","execution":{"iopub.status.busy":"2022-06-17T06:24:24.898396Z","iopub.execute_input":"2022-06-17T06:24:24.899214Z","iopub.status.idle":"2022-06-17T06:24:24.909227Z","shell.execute_reply.started":"2022-06-17T06:24:24.899176Z","shell.execute_reply":"2022-06-17T06:24:24.908415Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"> Besides, we will try to create a new feature \"Distance_to_station\" to see if it has any impact on the house prices or not. The code will be implemented in a separate .ipynb file.","metadata":{"id":"Br6yt1ASB2b5"}},{"cell_type":"markdown","source":"# 4. Missing Data & Cleaning\nWe have 6 features with missing data: Landsize, Car, Bathroom, Bedroom, Longtitude, Lattitude.","metadata":{"id":"zXWHsiXY0xWc"}},{"cell_type":"code","source":"transformer(show_nan=True,figsize=(9,5),title='Feature (NaN) %').transform(X=df_EDA)","metadata":{"id":"WyO9uHCd6-S6","outputId":"2c7d3cea-8a31-4b68-8fef-c024d9b13a1c","execution":{"iopub.status.busy":"2022-06-17T06:24:24.910706Z","iopub.execute_input":"2022-06-17T06:24:24.911387Z","iopub.status.idle":"2022-06-17T06:24:25.235421Z","shell.execute_reply.started":"2022-06-17T06:24:24.911346Z","shell.execute_reply":"2022-06-17T06:24:25.234655Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### **Landsize** Imputation\n\nFirstly, we will remove records whose land sizes of less than 10 square meters are removed. They are outliers which may stem from factors such as human errors, relationship with probability models, and even structured situations.","metadata":{"id":"rGqUZKjH1CjL"}},{"cell_type":"code","source":"df_EDA.shape","metadata":{"id":"i6dXDFaG2Jj6","outputId":"9596f4ba-3399-45d2-e0a4-4daf53e60bef","execution":{"iopub.status.busy":"2022-06-17T06:24:25.237276Z","iopub.execute_input":"2022-06-17T06:24:25.238214Z","iopub.status.idle":"2022-06-17T06:24:25.244712Z","shell.execute_reply.started":"2022-06-17T06:24:25.238159Z","shell.execute_reply":"2022-06-17T06:24:25.243782Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"df_EDA['Landsize'].sort_values()","metadata":{"id":"Px5m7CP0zQEH","outputId":"34444302-30eb-4ed5-ec65-4e29799e8faf","execution":{"iopub.status.busy":"2022-06-17T06:24:25.245760Z","iopub.execute_input":"2022-06-17T06:24:25.247141Z","iopub.status.idle":"2022-06-17T06:24:25.262141Z","shell.execute_reply.started":"2022-06-17T06:24:25.247103Z","shell.execute_reply":"2022-06-17T06:24:25.261365Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df_EDA = df_EDA[df_EDA['Landsize'] >= 10]","metadata":{"id":"4iD4uNm512TP","execution":{"iopub.status.busy":"2022-06-17T06:24:25.264022Z","iopub.execute_input":"2022-06-17T06:24:25.264358Z","iopub.status.idle":"2022-06-17T06:24:25.273010Z","shell.execute_reply.started":"2022-06-17T06:24:25.264325Z","shell.execute_reply":"2022-06-17T06:24:25.272113Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"df_EDA.Landsize","metadata":{"id":"Do3R_51t2NDJ","outputId":"65ca0e18-1193-482e-f28e-694cafb1d1a3","execution":{"iopub.status.busy":"2022-06-17T06:24:25.274592Z","iopub.execute_input":"2022-06-17T06:24:25.275126Z","iopub.status.idle":"2022-06-17T06:24:25.285343Z","shell.execute_reply.started":"2022-06-17T06:24:25.275086Z","shell.execute_reply":"2022-06-17T06:24:25.284521Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"As we have some huge Land size value then it will affect the mean land size, we will impute the Land size values by using its median values group by house types and suburbs.","metadata":{"id":"eBbG_VK32u2-"}},{"cell_type":"code","source":"df_EDA['Landsize'] = df_EDA['Landsize'].fillna(df_EDA.groupby(['Type','Suburb'])['Landsize'].transform('median'))","metadata":{"id":"CWmb91zY2sbw","execution":{"iopub.status.busy":"2022-06-17T06:24:25.286908Z","iopub.execute_input":"2022-06-17T06:24:25.287540Z","iopub.status.idle":"2022-06-17T06:24:25.301295Z","shell.execute_reply.started":"2022-06-17T06:24:25.287477Z","shell.execute_reply":"2022-06-17T06:24:25.300304Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"df_EDA['Landsize'].isna().sum()","metadata":{"id":"wgdb3Sh94gFj","outputId":"95765262-f2bc-49b2-f3cd-419cf06e71ec","execution":{"iopub.status.busy":"2022-06-17T06:24:25.302446Z","iopub.execute_input":"2022-06-17T06:24:25.302910Z","iopub.status.idle":"2022-06-17T06:24:25.309705Z","shell.execute_reply.started":"2022-06-17T06:24:25.302872Z","shell.execute_reply":"2022-06-17T06:24:25.308755Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"df_EDA.isna().sum()","metadata":{"id":"EDxZWhlQ4j35","outputId":"500a720c-291c-4c56-8323-b4b7d34551df","execution":{"iopub.status.busy":"2022-06-17T06:24:25.313436Z","iopub.execute_input":"2022-06-17T06:24:25.314270Z","iopub.status.idle":"2022-06-17T06:24:25.336440Z","shell.execute_reply.started":"2022-06-17T06:24:25.314228Z","shell.execute_reply":"2022-06-17T06:24:25.335767Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### **Bedroom**, **Bathroom**, **Car** Imputation\n\nWe will impute the **Bedroom**, **Bathroom**, **Car** values by using its median values group by house types.","metadata":{"id":"44Dh5wr05Gjw"}},{"cell_type":"code","source":"df_EDA['Bedrooms'] = df_EDA['Bedrooms'].fillna(df_EDA.groupby('Type')['Bedrooms'].transform('median'))\ndf_EDA['Bathroom'] = df_EDA['Bathroom'].fillna(df_EDA.groupby('Type')['Bathroom'].transform('median'))\ndf_EDA['Car'] = df_EDA['Car'].fillna(df_EDA.groupby('Type')['Car'].transform('median'))","metadata":{"id":"KeAy0z-U4sLA","execution":{"iopub.status.busy":"2022-06-17T06:24:25.337488Z","iopub.execute_input":"2022-06-17T06:24:25.338129Z","iopub.status.idle":"2022-06-17T06:24:25.355569Z","shell.execute_reply.started":"2022-06-17T06:24:25.338091Z","shell.execute_reply":"2022-06-17T06:24:25.354666Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"df_EDA.isna().sum()","metadata":{"id":"O5CeYEE665vt","outputId":"50aab2ed-3f37-432b-8fa3-4234f6461dc3","execution":{"iopub.status.busy":"2022-06-17T06:24:25.356785Z","iopub.execute_input":"2022-06-17T06:24:25.357519Z","iopub.status.idle":"2022-06-17T06:24:25.378308Z","shell.execute_reply.started":"2022-06-17T06:24:25.357480Z","shell.execute_reply":"2022-06-17T06:24:25.377218Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### **Lattitude**, **Longtitude** Imputation\nWe will try to fill null values in these features from house addresses using GeoPy. \n","metadata":{"id":"tYphUPna9fto"}},{"cell_type":"code","source":"# !pip install geopy","metadata":{"id":"HNYHRoEUKu5X","execution":{"iopub.status.busy":"2022-06-17T06:24:25.379627Z","iopub.execute_input":"2022-06-17T06:24:25.380567Z","iopub.status.idle":"2022-06-17T06:24:25.384859Z","shell.execute_reply.started":"2022-06-17T06:24:25.380522Z","shell.execute_reply":"2022-06-17T06:24:25.383958Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#!pip install plotly","metadata":{"id":"w5U4UlwQKu5X","execution":{"iopub.status.busy":"2022-06-17T06:24:25.386223Z","iopub.execute_input":"2022-06-17T06:24:25.387103Z","iopub.status.idle":"2022-06-17T06:24:25.393950Z","shell.execute_reply.started":"2022-06-17T06:24:25.387060Z","shell.execute_reply":"2022-06-17T06:24:25.392991Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter\n \n#Creating a dataframe with address of locations we want to reterive\ncondition = df_EDA['Longtitude'].isna()\ndf_address = pd.DataFrame((df_EDA[condition]['Address']))\ndf_address['Full_address'] = df_EDA[condition]['Address'] + ' ' + df_EDA[condition].Suburb\n# df_address[\"Address\"] = df_address[\"Address\"] + df_EDA[condition].Suburb\n# df_address.columns = \n\n# #Creating an instance of Nominatim Class\ngeolocator = Nominatim(user_agent=\"my_request\")\n \n# #applying the rate limiter wrapper\ngeocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n \n# #Applying the method to pandas DataFrame\ndf_address['location'] = df_address['Full_address'].apply(geocode)\ndf_address['Lat'] = df_address['location'].apply(lambda x: x.latitude if x else None)\ndf_address['Lon'] = df_address['location'].apply(lambda x: x.longitude if x else None)","metadata":{"id":"BHe3xOp_-0OP","execution":{"iopub.status.busy":"2022-06-17T06:24:25.396802Z","iopub.execute_input":"2022-06-17T06:24:25.397308Z","iopub.status.idle":"2022-06-17T06:24:43.738322Z","shell.execute_reply.started":"2022-06-17T06:24:25.397267Z","shell.execute_reply":"2022-06-17T06:24:43.737517Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"df_EDA['Lattitude'] = df_EDA['Lattitude'].combine_first(df_EDA['Address'].map(df_address.set_index('Address')['Lat']))\ndf_EDA['Longtitude'] = df_EDA['Longtitude'].combine_first(df_EDA['Address'].map(df_address.set_index('Address')['Lon']))","metadata":{"id":"9yxUJfKO08oC","execution":{"iopub.status.busy":"2022-06-17T06:24:43.739747Z","iopub.execute_input":"2022-06-17T06:24:43.740117Z","iopub.status.idle":"2022-06-17T06:24:43.756762Z","shell.execute_reply.started":"2022-06-17T06:24:43.740082Z","shell.execute_reply":"2022-06-17T06:24:43.755819Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# 5. EXPLORATORY DATA ANALYSIS","metadata":{"id":"Fy3lr_zk-f3y"}},{"cell_type":"code","source":"main_df = pd.read_csv ('../input/preprocessed-data/preprocessed_data.csv').drop(columns = ['Unnamed: 0'])","metadata":{"id":"WZgpGG5580Zs","execution":{"iopub.status.busy":"2022-06-17T06:24:43.758169Z","iopub.execute_input":"2022-06-17T06:24:43.758819Z","iopub.status.idle":"2022-06-17T06:24:43.859491Z","shell.execute_reply.started":"2022-06-17T06:24:43.758777Z","shell.execute_reply":"2022-06-17T06:24:43.858659Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"main_df","metadata":{"id":"JOjgbb3jKu5Z","outputId":"73ce6876-c5d8-4081-9f1f-17c033e0eba2","execution":{"iopub.status.busy":"2022-06-17T06:24:43.898170Z","iopub.execute_input":"2022-06-17T06:24:43.898685Z","iopub.status.idle":"2022-06-17T06:24:43.938993Z","shell.execute_reply.started":"2022-06-17T06:24:43.898648Z","shell.execute_reply":"2022-06-17T06:24:43.938267Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"### **5.1. Categorical EDA**","metadata":{"id":"QrkOPYTwwrDn"}},{"cell_type":"code","source":"#Type, Method, Region name","metadata":{"id":"yTP2uhTbwrDo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nimport plotly.offline as pyo\npyo.init_notebook_mode()","metadata":{"id":"93LqKzQtwrDp","outputId":"a2db412d-3362-4fa9-8cee-199808b9529c","execution":{"iopub.status.busy":"2022-06-17T06:24:43.940013Z","iopub.execute_input":"2022-06-17T06:24:43.940661Z","iopub.status.idle":"2022-06-17T06:24:43.993465Z","shell.execute_reply.started":"2022-06-17T06:24:43.940626Z","shell.execute_reply":"2022-06-17T06:24:43.992553Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"*Among 3 type of accomodation (t, u, h), type_h appears to be more slightly expensive than other*","metadata":{"id":"kw0AIZNOwrDp"}},{"cell_type":"code","source":"fig = px.box(main_df, y=\"Price\", x = 'Type')\nfig.show()","metadata":{"id":"vZbYKCPhwrDp","outputId":"5e5913b1-aa67-4173-a686-b558f921d3af","execution":{"iopub.status.busy":"2022-06-17T06:24:55.118544Z","iopub.execute_input":"2022-06-17T06:24:55.118913Z","iopub.status.idle":"2022-06-17T06:24:55.298853Z","shell.execute_reply.started":"2022-06-17T06:24:55.118882Z","shell.execute_reply":"2022-06-17T06:24:55.297983Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"fig = px.box(main_df, y = \"Price\", x = \"Method\")\nfig.show()","metadata":{"id":"e_Mr62AxwrDq","outputId":"5b75f544-62db-4381-b861-dd30e9dbf5eb","execution":{"iopub.status.busy":"2022-06-17T06:24:57.239752Z","iopub.execute_input":"2022-06-17T06:24:57.240572Z","iopub.status.idle":"2022-06-17T06:24:57.417515Z","shell.execute_reply.started":"2022-06-17T06:24:57.240537Z","shell.execute_reply":"2022-06-17T06:24:57.416652Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"fig = px.box(main_df, y = \"Price\", x = \"Regionname\")\nfig.show()","metadata":{"id":"sX6qOKHBwrDq","outputId":"a19e826f-3eaa-465b-a57e-31058fa5f561","execution":{"iopub.status.busy":"2022-06-17T06:24:59.096955Z","iopub.execute_input":"2022-06-17T06:24:59.097729Z","iopub.status.idle":"2022-06-17T06:24:59.280134Z","shell.execute_reply.started":"2022-06-17T06:24:59.097692Z","shell.execute_reply":"2022-06-17T06:24:59.279467Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"### *Top 10 Suburb that have the largest median of price of house*","metadata":{"id":"jfXSK1ZpwrDq"}},{"cell_type":"code","source":"top_price = main_df.groupby ('Suburb').median(['Price']).reset_index().sort_values (by = ['Price'], ascending = False).reset_index().drop(columns = ['index']).head(10)\nbox_top_price_data = main_df.loc[main_df.Suburb.isin(top_price.Suburb.to_list())]","metadata":{"id":"DljfKwZ1wrDq","execution":{"iopub.status.busy":"2022-06-17T06:25:05.199191Z","iopub.execute_input":"2022-06-17T06:25:05.199583Z","iopub.status.idle":"2022-06-17T06:25:05.222205Z","shell.execute_reply.started":"2022-06-17T06:25:05.199549Z","shell.execute_reply":"2022-06-17T06:25:05.221275Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"box_top_price_data","metadata":{"id":"l-NqGQMqwrDr","outputId":"18ddbef1-de6a-42e5-ffd8-9d999f24c226","execution":{"iopub.status.busy":"2022-06-17T06:25:06.384163Z","iopub.execute_input":"2022-06-17T06:25:06.384683Z","iopub.status.idle":"2022-06-17T06:25:06.426599Z","shell.execute_reply.started":"2022-06-17T06:25:06.384646Z","shell.execute_reply":"2022-06-17T06:25:06.425855Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"fig = px.box(main_df, y = \"Price\", x = \"Suburb\")\n\nfig.show()","metadata":{"id":"BHBU73rhwrDr","outputId":"7e89a18d-6b8f-4d69-c0aa-e54719497a21","execution":{"iopub.status.busy":"2022-06-17T06:25:06.788165Z","iopub.execute_input":"2022-06-17T06:25:06.788815Z","iopub.status.idle":"2022-06-17T06:25:06.969285Z","shell.execute_reply.started":"2022-06-17T06:25:06.788777Z","shell.execute_reply":"2022-06-17T06:25:06.968534Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"fig = px.box(box_top_price_data, y = \"Price\", x = \"Suburb\")\nfig.show()","metadata":{"id":"iTMvL47-wrDr","outputId":"2b1f8448-705c-4b30-d28f-20158e9ebe06","execution":{"iopub.status.busy":"2022-06-17T06:25:07.178315Z","iopub.execute_input":"2022-06-17T06:25:07.179006Z","iopub.status.idle":"2022-06-17T06:25:07.240840Z","shell.execute_reply.started":"2022-06-17T06:25:07.178965Z","shell.execute_reply":"2022-06-17T06:25:07.239924Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"### **5.2. Houses sold group by *['type', 'suburb']***","metadata":{"id":"H1Vcs0TfwrDs"}},{"cell_type":"code","source":"count_type_suburb = main_df.groupby(['Suburb', 'Type'])['Address'].count().reset_index()\ncount_type_suburb.rename ({'Address':'Sold_Count'}, axis = 1, inplace = True)\ncount_type_suburb_sum = count_type_suburb.groupby (['Suburb'])['Sold_Count'].sum().reset_index()\ncount_type_suburb_sum = count_type_suburb_sum.sort_values (['Sold_Count'], ascending = False).reset_index()\ncount_type_suburb_sum.drop(columns = ['index', 'Sold_Count'], inplace = True)\ncount_type_suburb_sum['Order'] = [i for i in range (len(count_type_suburb_sum))]\ncount_type_suburb_sum","metadata":{"id":"s9Lq97F8wrDs","outputId":"379575dc-1fba-4fe6-9cfe-deef4ab1b956","execution":{"iopub.status.busy":"2022-06-17T06:25:08.288104Z","iopub.execute_input":"2022-06-17T06:25:08.288449Z","iopub.status.idle":"2022-06-17T06:25:08.315701Z","shell.execute_reply.started":"2022-06-17T06:25:08.288420Z","shell.execute_reply":"2022-06-17T06:25:08.314833Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"sorted_count_type_suburb = count_type_suburb_sum.merge(count_type_suburb, how = 'right', on = ['Suburb'])\nsorted_count_type_suburb = sorted_count_type_suburb.sort_values (['Order'])\nsorted_count_type_suburb = sorted_count_type_suburb.reset_index().drop(columns = ['index'])\nsorted_count_type_suburb\n","metadata":{"id":"f774UDVRwrDs","outputId":"f2fe2867-d58a-4707-b3d5-119a67e77cc1","execution":{"iopub.status.busy":"2022-06-17T06:25:08.962154Z","iopub.execute_input":"2022-06-17T06:25:08.962537Z","iopub.status.idle":"2022-06-17T06:25:08.988141Z","shell.execute_reply.started":"2022-06-17T06:25:08.962503Z","shell.execute_reply":"2022-06-17T06:25:08.987304Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"#stacked bar?? should be in a specific period of time.\n\nfig = px.bar(sorted_count_type_suburb, x=\"Suburb\", y=\"Sold_Count\", color=\"Type\", title=\"Stacked-Histogram: Count of sold by Suburb & Type\")\nfig.show()","metadata":{"id":"qVrMVWUOwrDs","outputId":"6d4a0b51-0801-41e4-bfd9-4f0b18b3d80a","execution":{"iopub.status.busy":"2022-06-17T06:25:09.671037Z","iopub.execute_input":"2022-06-17T06:25:09.671796Z","iopub.status.idle":"2022-06-17T06:25:09.748368Z","shell.execute_reply.started":"2022-06-17T06:25:09.671760Z","shell.execute_reply":"2022-06-17T06:25:09.747513Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"### **5.3. Numberical EDA**","metadata":{"id":"EpVJbAEbwrDt"}},{"cell_type":"code","source":"main_df_numer = main_df.loc[:, ['Address','Price','Bathroom', 'Distance', 'Bedrooms', 'Car', 'Rooms', 'Distance_to_Station', 'Landsize', 'main_lon', 'main_lat']]","metadata":{"id":"uO5othpmKu5c","execution":{"iopub.status.busy":"2022-06-17T06:25:13.698165Z","iopub.execute_input":"2022-06-17T06:25:13.698990Z","iopub.status.idle":"2022-06-17T06:25:13.706647Z","shell.execute_reply.started":"2022-06-17T06:25:13.698953Z","shell.execute_reply":"2022-06-17T06:25:13.704691Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"#### *Features correlation*","metadata":{"id":"usrAEB0-Ku5c"}},{"cell_type":"code","source":"plt.figure (figsize = (10,5))\nplt.title ('Features correlation betweens numerical features')\nsns.heatmap(main_df_numer.corr().round(2), annot = True)","metadata":{"id":"kTy2Lz2uKu5c","outputId":"31cc2d67-7659-4ab1-db47-7f008f6cae62","execution":{"iopub.status.busy":"2022-06-17T06:25:18.146015Z","iopub.execute_input":"2022-06-17T06:25:18.146374Z","iopub.status.idle":"2022-06-17T06:25:18.949223Z","shell.execute_reply.started":"2022-06-17T06:25:18.146337Z","shell.execute_reply":"2022-06-17T06:25:18.948445Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"> From the correlation matrix above, we can easily recognize that: \n\n> Bathrooms, Bedrooms, Total number of rooms, Distance to CBD have strong correlation to Price, while Car-space, Distance to Station, and Landsize have less positive impact to house price","metadata":{"id":"U55AQs1YKu5d"}},{"cell_type":"markdown","source":"#### *Scatter plots*","metadata":{"id":"40g1CcRWwrDt"}},{"cell_type":"code","source":"main_df_numer.describe().round (2)","metadata":{"id":"zugFaEWFwrDt","outputId":"3c89fc5c-c4e3-49db-8d24-b3650ddb5674","execution":{"iopub.status.busy":"2022-06-17T06:25:23.399285Z","iopub.execute_input":"2022-06-17T06:25:23.399847Z","iopub.status.idle":"2022-06-17T06:25:23.455327Z","shell.execute_reply.started":"2022-06-17T06:25:23.399801Z","shell.execute_reply":"2022-06-17T06:25:23.454443Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"main_df_numer = main_df.loc[:, ['Address','Price','Bathroom', 'Distance', 'Bedrooms', 'Car', 'Rooms', 'Distance_to_Station', 'Landsize', 'main_lon', 'main_lat']]","metadata":{"id":"2eHPJ50zwrDt","execution":{"iopub.status.busy":"2022-06-17T06:25:23.889970Z","iopub.execute_input":"2022-06-17T06:25:23.890554Z","iopub.status.idle":"2022-06-17T06:25:23.899843Z","shell.execute_reply.started":"2022-06-17T06:25:23.890508Z","shell.execute_reply":"2022-06-17T06:25:23.898479Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objs as go\n\nfrom plotly.subplots import make_subplots\ntitles = ['Bathroom', 'Distance', 'Bedrooms', 'Car space', 'Rooms', 'Distance to Station']\nfig = make_subplots(rows=3, cols=2,shared_yaxes=True,subplot_titles=titles,horizontal_spacing = 0.01, vertical_spacing = 0.06)\n\nfig.add_trace(go.Scattergl(y=main_df_numer['Price'].values,x=main_df_numer['Bathroom'].values,mode='markers',name='Bathroom',text=main_df.index,opacity=0.5),row=1, col=1)\nfig.add_trace(go.Scattergl(y=main_df_numer['Price'].values,x=main_df_numer['Distance'].values,mode='markers',name='Distance',text=main_df.index,opacity=0.1),row=1, col=2)\nfig.add_trace(go.Scattergl(y=main_df_numer['Price'].values,x=main_df_numer['Bedrooms'].values,mode='markers',name='Bedrooms',text=main_df.index,opacity=0.1),row=2, col=1)\nfig.add_trace(go.Scattergl(y=main_df_numer['Price'].values,x=main_df_numer['Car'].values,mode='markers',name='Car',text=main_df.index,opacity=0.1),row=2, col=2)\nfig.add_trace(go.Scattergl(y=main_df_numer['Price'].values,x=main_df_numer['Rooms'].values,mode='markers',name='Rooms',text=main_df.index,opacity=0.1),row=3, col=1)\nfig.add_trace(go.Scattergl(y=main_df_numer['Price'].values,x=main_df_numer['Distance_to_Station'].values,mode='markers',name='Distance to station',text=main_df.index,opacity=0.1),row=3, col=2)\n\n\nfig.update_traces(marker=dict(size=4,line=dict(width=1.2,color='black')))\nfig.update_layout(template='plotly_white',title={'text':'Important features relative to Price', 'xanchor': 'center', 'x':0.5},height=1000,showlegend= False)\nfig.update_layout(margin={\"r\":0,\"t\":60,\"l\":0,\"b\":0})\nfig.show()","metadata":{"id":"4LcGGsmgwrDu","outputId":"1e7bef24-aa32-4ab2-c0e6-81233438383f","execution":{"iopub.status.busy":"2022-06-17T06:25:25.309269Z","iopub.execute_input":"2022-06-17T06:25:25.309649Z","iopub.status.idle":"2022-06-17T06:25:25.865503Z","shell.execute_reply.started":"2022-06-17T06:25:25.309617Z","shell.execute_reply":"2022-06-17T06:25:25.864404Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"#### Comment:\n- Most houses have less than 5 bedrooms, 5 bathrooms and 5 rooms in total\n\n- Most houses houses are under 30km to the CBD and 10km to the station in the suburbs\n\n- Among 6 features above, 4 of them have positive correlation with Price, while Distance to CBD and Ditance to Station have negative one.","metadata":{"id":"RwEiUXNLwrDu"}},{"cell_type":"markdown","source":"### 5.4 Geospartial plot","metadata":{"id":"DDaiQe9gwrDu"}},{"cell_type":"code","source":"#sort data by name of suburb\ngeo_df = main_df.sort_values (['Suburb']).sample(2000).reset_index().drop(columns = ['index'])\ngeo_df","metadata":{"id":"LmV8zJ4uwrDv","outputId":"b26d179f-43ff-4546-fb43-7c1c164ac638","execution":{"iopub.status.busy":"2022-06-17T06:25:29.702632Z","iopub.execute_input":"2022-06-17T06:25:29.702993Z","iopub.status.idle":"2022-06-17T06:25:29.765027Z","shell.execute_reply.started":"2022-06-17T06:25:29.702962Z","shell.execute_reply":"2022-06-17T06:25:29.764290Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"import folium\n\nm = folium.Map(location=[-37.840935, 144.946457], zoom_start=15)\ntooltip = \"Click Here For More Info\"\n\nfor i in range (len (main_df[:2000])):\n    marker = folium.Marker (\n    location = [main_df['main_lat'][i], main_df['main_lon'][i]],\n                popup = '<stong>Nothing to display</stong>',\n                tooltip = [main_df['Address'][i], main_df['Price'][i]])\n    \n    marker.add_to (m)\n\nm","metadata":{"id":"xZ-rTxB9wrDv","outputId":"b0f405f0-381f-4807-952e-200b533d3847","execution":{"iopub.status.busy":"2022-06-17T06:25:33.235248Z","iopub.execute_input":"2022-06-17T06:25:33.235754Z","iopub.status.idle":"2022-06-17T06:25:37.259954Z","shell.execute_reply.started":"2022-06-17T06:25:33.235719Z","shell.execute_reply":"2022-06-17T06:25:37.259230Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"*Selling houses are located in clusters and mostly close the railway and stations*","metadata":{"id":"F7OGIr17wrDv"}},{"cell_type":"markdown","source":"# 6. Data Preparation For Modeling","metadata":{"id":"Cy_tEPWhwrDv"}},{"cell_type":"markdown","source":"### *6.1. Removing outliers*","metadata":{"id":"3OqJoCqRwrDv"}},{"cell_type":"markdown","source":"Look at the scatter plots above, we can easily see that there are so many outliers. They need to be removed before we apply the data into training set!","metadata":{"id":"w92rT4xqwrDv"}},{"cell_type":"code","source":"main_df","metadata":{"id":"9gaJIgWcKu5g","outputId":"cf61857a-d387-4f47-8c9d-6195e8111dab","execution":{"iopub.status.busy":"2022-06-17T06:25:39.075310Z","iopub.execute_input":"2022-06-17T06:25:39.076298Z","iopub.status.idle":"2022-06-17T06:25:39.117320Z","shell.execute_reply.started":"2022-06-17T06:25:39.076260Z","shell.execute_reply":"2022-06-17T06:25:39.116402Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"#using Z_score\nfrom scipy import stats\nz_value = np.abs (stats.zscore (main_df_numer.iloc[:, 1:]))\nz_value","metadata":{"id":"I6s5e2kywrDv","outputId":"8e4d1f03-fa80-4507-92af-5ecadcb90cff","execution":{"iopub.status.busy":"2022-06-17T06:25:39.512195Z","iopub.execute_input":"2022-06-17T06:25:39.512909Z","iopub.status.idle":"2022-06-17T06:25:39.543721Z","shell.execute_reply.started":"2022-06-17T06:25:39.512856Z","shell.execute_reply":"2022-06-17T06:25:39.542909Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"#Clear out data that lay out of the range [-3 < z < 3]\nmain_df_numer_after_z = main_df_numer[(z_value < 3).all(axis = 1)]\nmain_df_numer_after_z","metadata":{"id":"yG_CgHbzwrDw","outputId":"26b60ed9-9da0-493b-bad0-c45edf636dc5","execution":{"iopub.status.busy":"2022-06-17T06:25:39.996118Z","iopub.execute_input":"2022-06-17T06:25:39.996819Z","iopub.status.idle":"2022-06-17T06:25:40.028771Z","shell.execute_reply.started":"2022-06-17T06:25:39.996780Z","shell.execute_reply":"2022-06-17T06:25:40.027887Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"from plotly.subplots import make_subplots\ntitles = ['Bathroom', 'Distance', 'Bedrooms', 'Car space', 'Rooms', 'Distance to Station']\nfig = make_subplots(rows=3, cols=2,shared_yaxes=True,subplot_titles=titles,horizontal_spacing = 0.01, vertical_spacing = 0.06)\n\nfig.add_trace(go.Scattergl(y=main_df_numer_after_z['Price'].values,x=main_df_numer_after_z['Bathroom'].values,mode='markers',name='Bathroom',text=main_df.index,opacity=0.5),row=1, col=1)\nfig.add_trace(go.Scattergl(y=main_df_numer_after_z['Price'].values,x=main_df_numer_after_z['Distance'].values,mode='markers',name='Distance',text=main_df.index,opacity=0.1),row=1, col=2)\nfig.add_trace(go.Scattergl(y=main_df_numer_after_z['Price'].values,x=main_df_numer_after_z['Bedrooms'].values,mode='markers',name='Bedrooms',text=main_df.index,opacity=0.1),row=2, col=1)\nfig.add_trace(go.Scattergl(y=main_df_numer_after_z['Price'].values,x=main_df_numer_after_z['Car'].values,mode='markers',name='Car',text=main_df.index,opacity=0.1),row=2, col=2)\nfig.add_trace(go.Scattergl(y=main_df_numer_after_z['Price'].values,x=main_df_numer_after_z['Rooms'].values,mode='markers',name='Rooms',text=main_df.index,opacity=0.1),row=3, col=1)\nfig.add_trace(go.Scattergl(y=main_df_numer_after_z['Price'].values,x=main_df_numer_after_z['Distance_to_Station'].values,mode='markers',name='Distance to station',text=main_df.index,opacity=0.1),row=3, col=2)\n\n\nfig.update_traces(marker=dict(size=4,line=dict(width=1.2,color='black')))\nfig.update_layout(template='plotly_white',title={'text':'Important features relative to Price', 'xanchor': 'center', 'x':0.5},height=1000,showlegend= False)\nfig.update_layout(margin={\"r\":0,\"t\":60,\"l\":0,\"b\":0})\nfig.show()","metadata":{"id":"uy10LNYzwrDw","outputId":"e6539cae-7021-435c-9014-ed0aaefe52a1","execution":{"iopub.status.busy":"2022-06-17T06:25:42.725258Z","iopub.execute_input":"2022-06-17T06:25:42.725646Z","iopub.status.idle":"2022-06-17T06:25:43.046865Z","shell.execute_reply.started":"2022-06-17T06:25:42.725614Z","shell.execute_reply":"2022-06-17T06:25:43.044552Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"*Data appears to have less outliers on both sides*","metadata":{"id":"wOSkw5GjwrDw"}},{"cell_type":"markdown","source":"### *6.2. Features correlation after outliers removal*","metadata":{"id":"80fhufCWKu5h"}},{"cell_type":"code","source":"plt.figure (figsize = (10, 5))\nplt.title ('Features correlation after removing outliers')\nsns.heatmap (main_df_numer_after_z.corr().round(2), annot = True)","metadata":{"id":"uPzubcMMKu5h","outputId":"049bf09c-61ff-4be7-fac1-a72016e10f3a","execution":{"iopub.status.busy":"2022-06-17T06:25:44.522164Z","iopub.execute_input":"2022-06-17T06:25:44.522561Z","iopub.status.idle":"2022-06-17T06:25:45.126027Z","shell.execute_reply.started":"2022-06-17T06:25:44.522527Z","shell.execute_reply":"2022-06-17T06:25:45.125264Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"> After scaling data with Normal distribution, Bathrooms, Distance, Bedrooms, Total rooms, Longtitude and Lattitude seem to have stronger affects on Price, unless Distance to nearest station perform worse than before!","metadata":{"id":"YXwb2f_jKu5i"}},{"cell_type":"markdown","source":"### *6.3. Clean unrelated data*","metadata":{"id":"ZQqSfkmuwrDw"}},{"cell_type":"markdown","source":"- *Postcode*, *Suburb*, *Council Area*, *Regionname* are 4 features that equally and significantly affect Price. However, these are categorical data and instead, we can use Longtitude and Lattitude (numerical data) as alternative features, then we could drop those 4 features.\n- *Method*, *Seller* should also be dropped because they seems to have no effect on Price","metadata":{"id":"Nc08niHRO2Lg"}},{"cell_type":"code","source":"output_df = main_df.merge (main_df_numer_after_z, on = ['Address', 'Price', 'Bathroom', 'Distance', 'Bedrooms', 'Car', 'Rooms', 'Distance_to_Station', 'Landsize'], how = 'right')\n\noutput_main = main_df.merge (main_df_numer_after_z, on = ['Address', 'Price', 'Bathroom', 'Distance', 'Bedrooms', 'Car', 'Rooms', 'Distance_to_Station', 'Landsize', 'main_lon', 'main_lat'], how = 'right')\n\nuse_data = output_main.drop(columns = ['Suburb', 'Address', 'Method', 'SellerG', 'CouncilArea', 'Regionname', 'station', 'Postcode', 'Propertycount', 'Sold_Year', 'Sold_Month', 'metro_lat', 'metro_lon'])\nuse_data.head()","metadata":{"id":"eiyRNd6dwrDw","outputId":"f0b5d165-05b0-47c9-f283-6d3305ec6c8f","execution":{"iopub.status.busy":"2022-06-17T06:25:45.951173Z","iopub.execute_input":"2022-06-17T06:25:45.951998Z","iopub.status.idle":"2022-06-17T06:25:46.097095Z","shell.execute_reply.started":"2022-06-17T06:25:45.951961Z","shell.execute_reply":"2022-06-17T06:25:46.096308Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"### *6.3. Feature Importance*","metadata":{"id":"jJ0zgc4hFQAh"}},{"cell_type":"code","source":"transformer(feature_importance=True,figsize=(800,400),target='Price').transform(X=use_data)","metadata":{"id":"cLuS2tybFS84","outputId":"0da25b56-9ea1-4de5-aa5e-f29f1acc5896","execution":{"iopub.status.busy":"2022-06-17T06:25:48.444174Z","iopub.execute_input":"2022-06-17T06:25:48.444568Z","iopub.status.idle":"2022-06-17T06:26:28.460856Z","shell.execute_reply.started":"2022-06-17T06:25:48.444536Z","shell.execute_reply":"2022-06-17T06:26:28.460048Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"### *6.4. Normalization & Categorical Encoder*\n*Using Standard Normalization*","metadata":{"id":"WRozwiVTwrDw"}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n#create dummies from \"type\"\ndummy_type = pd.get_dummies(use_data[['Type']])\nuse_data = dummy_type.merge (use_data, left_index = True, right_index = True).drop(columns = ['Type'])\nuse_data.head(5)\n\nfeatures = use_data.drop(columns = ['Price'])\nprice_ = use_data['Price']\n\nfeatures_ = StandardScaler().fit_transform (features)\nfeatures_","metadata":{"id":"MbM5MRbDwrDx","outputId":"5819afc8-002a-4f1b-ad81-a03225db838d","execution":{"iopub.status.busy":"2022-06-17T06:26:28.464505Z","iopub.execute_input":"2022-06-17T06:26:28.464982Z","iopub.status.idle":"2022-06-17T06:26:28.502106Z","shell.execute_reply.started":"2022-06-17T06:26:28.464936Z","shell.execute_reply":"2022-06-17T06:26:28.501060Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"# 7. BUILDING MODELS","metadata":{"id":"uWBoymMXHwZL"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features_, price_, test_size=0.2, random_state= 42)","metadata":{"id":"PcU-djh4PxWH","execution":{"iopub.status.busy":"2022-06-17T06:26:28.506498Z","iopub.execute_input":"2022-06-17T06:26:28.508875Z","iopub.status.idle":"2022-06-17T06:26:28.521597Z","shell.execute_reply.started":"2022-06-17T06:26:28.508833Z","shell.execute_reply":"2022-06-17T06:26:28.520256Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"id":"A0lPapqYKu5j","outputId":"64bd1f8e-67c2-48c9-ef12-0bb6b204edf3","execution":{"iopub.status.busy":"2022-06-17T06:26:28.524073Z","iopub.execute_input":"2022-06-17T06:26:28.525566Z","iopub.status.idle":"2022-06-17T06:26:28.546981Z","shell.execute_reply.started":"2022-06-17T06:26:28.525522Z","shell.execute_reply":"2022-06-17T06:26:28.540227Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"# from catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor as GPR\nfrom sklearn.gaussian_process.kernels import ConstantKernel, RBF\nfrom sklearn.ensemble import (RandomForestRegressor,GradientBoostingRegressor,\n                              ExtraTreesRegressor,AdaBoostRegressor)\nfrom sklearn.linear_model import LinearRegression,Lasso,ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split # Data Split\nfrom sklearn.model_selection import cross_val_score # Cross Validation\nfrom sklearn.model_selection import StratifiedKFold # K-Fold Cross Validation\n\n''' Evaluate some promising models '''\n# let's look at how models perform overall, using default settings\n\nmodels = [] \nmodels.append(('LR',  LinearRegression()))\nmodels.append(('LASSO',Lasso()))\nmodels.append(('EN',ElasticNet()))  \nmodels.append(('KNN',KNeighborsRegressor()))        \nmodels.append(('CART',DecisionTreeRegressor()))     \nmodels.append(('ABR', AdaBoostRegressor()))\nmodels.append(('GBR', GradientBoostingRegressor()))\nmodels.append(('RFR', RandomForestRegressor(random_state=3)))\nmodels.append(('XGB', XGBRegressor(verbose_eval=False)))\n\n# Evaluate each model in turn using cross-validation\nresults, names = [], []\nfor name, model in models:\n\tkfold = StratifiedKFold(n_splits=10, random_state=3, shuffle=True)\n\tmodel_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n\tresults.append(model_results)\n\tnames.append(name)\n\tprint(name + \": \", model_results.mean(), model_results.std())\n\n#Compare algorithms using boxplots\nplt.figure(figsize=(15,8))\nplt.boxplot(results, labels=names)\nplt.title('Algorithm Comparison')\nplt.show()","metadata":{"id":"7HvN7PNcP5j1","outputId":"b338ce61-066e-4fb8-a663-c574955fb435","execution":{"iopub.status.busy":"2022-06-17T06:26:28.548367Z","iopub.execute_input":"2022-06-17T06:26:28.551078Z","iopub.status.idle":"2022-06-17T06:27:51.689086Z","shell.execute_reply.started":"2022-06-17T06:26:28.551033Z","shell.execute_reply":"2022-06-17T06:27:51.688319Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"#testing\nfrom sklearn import metrics\nscores = dict()\nfor name, model in models:\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    scores[name] = metrics.mean_squared_error(y_test, predictions)\n    print(name + 'MSE:' + str(metrics.mean_squared_error(y_test, predictions)))","metadata":{"id":"FQlBGV1aKu5k","outputId":"7cda806e-bb4b-4f0b-9f3a-4ac497b000c5","execution":{"iopub.status.busy":"2022-06-17T06:39:27.332052Z","iopub.execute_input":"2022-06-17T06:39:27.332519Z","iopub.status.idle":"2022-06-17T06:39:37.016344Z","shell.execute_reply.started":"2022-06-17T06:39:27.332438Z","shell.execute_reply":"2022-06-17T06:39:37.015652Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"> Our baseline models show no sign of overfitting, 'mse' at testing and trainning set are approximately similar!","metadata":{"id":"FkA0YMIyKu5k"}},{"cell_type":"markdown","source":"### *Should we use PCA to improve training time?*","metadata":{"id":"NEWy3T6_Ku5k"}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\ndef explained_var_ratio (number_of_components):\n    pca = PCA (n_components = number_of_components)\n    principalComponents = pca.fit_transform (X_train)\n    return np.sum(pca.explained_variance_ratio_)\n\nsum_ratio = []\nfor i in range (1, 13):\n    sum_ratio.append (explained_var_ratio (i))\n\nfig = px.line(x=[i for i in range(1, 13)], y=sum_ratio)\nfig.show()","metadata":{"id":"wczujAG3Ku5k","outputId":"33f65cf1-70a2-4e0d-f118-30f3985dd921","execution":{"iopub.status.busy":"2022-06-17T06:28:09.258392Z","iopub.execute_input":"2022-06-17T06:28:09.259191Z","iopub.status.idle":"2022-06-17T06:28:09.789062Z","shell.execute_reply.started":"2022-06-17T06:28:09.259157Z","shell.execute_reply":"2022-06-17T06:28:09.788313Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"#Choose n_components = 10 \npca = PCA (n_components = 10)\nprincipalComponents = pca.fit_transform (features_)","metadata":{"id":"TOmGOP5GKu5l","execution":{"iopub.status.busy":"2022-06-17T06:28:14.889471Z","iopub.execute_input":"2022-06-17T06:28:14.890122Z","iopub.status.idle":"2022-06-17T06:28:14.905713Z","shell.execute_reply.started":"2022-06-17T06:28:14.890083Z","shell.execute_reply":"2022-06-17T06:28:14.904600Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"#train-test split\nX_pca_train, X_pca_test, y_pca_train, y_pca_test = train_test_split(principalComponents, price_, test_size=0.2, random_state= 42)\n\nmodels_pca = [] \nmodels_pca.append(('LR',  LinearRegression()))\nmodels_pca.append(('LASSO',Lasso()))\nmodels_pca.append(('EN',ElasticNet()))  \nmodels_pca.append(('KNN',KNeighborsRegressor()))        \nmodels_pca.append(('CART',DecisionTreeRegressor()))     \nmodels_pca.append(('ABR', AdaBoostRegressor()))\nmodels_pca.append(('GBR', GradientBoostingRegressor()))\nmodels_pca.append(('RFR', RandomForestRegressor()))\nmodels_pca.append(('XGB', XGBRegressor(verbose_eval=False)))\n\n# Evaluate each model in turn using cross-validation\nresults_pca, names_pca = [], []\nfor name, model in models:\n\tkfold = StratifiedKFold(n_splits=10, random_state=3, shuffle=True)\n\tmodel_results = cross_val_score(model, X_pca_train, y_pca_train, cv=kfold, scoring='neg_mean_squared_error')\n\tresults_pca.append(model_results)\n\tnames_pca.append(name)\n\tprint(name + \": \", model_results.mean(), model_results.std())\n\n#Compare algorithms using boxplots\nplt.figure(figsize=(15,8))\nplt.boxplot(results_pca, labels=names)\nplt.title('Algorithm Comparison')\nplt.show()\n\n#testing\n\n#testing\nfrom sklearn import metrics\nfor name, model in models_pca[-3:]:\n    model.fit(X_pca_train, y_pca_train)\n    predictions = model.predict(X_pca_test)\n    print(name + 'MSE:' + str(metrics.mean_squared_error(y_pca_test, predictions)))\n","metadata":{"id":"b0AvoF2uKu5l","outputId":"4164747f-e5a5-43bc-bb80-d23a5f50869e","execution":{"iopub.status.busy":"2022-06-17T06:28:18.253444Z","iopub.execute_input":"2022-06-17T06:28:18.254244Z","iopub.status.idle":"2022-06-17T06:30:58.183740Z","shell.execute_reply.started":"2022-06-17T06:28:18.254208Z","shell.execute_reply":"2022-06-17T06:30:58.183124Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"*Comparing to the mse calculated with the initial train-test array, the mse when applying PCA is much higher.*\n\n*-> We will take the initial models.*","metadata":{"id":"jQTPtL8qKu5m"}},{"cell_type":"markdown","source":"## Hyperparameter tuning","metadata":{"id":"OkgzPLZjKu5n"}},{"cell_type":"markdown","source":"### *Random Forest Regressor*\nIn this grid search we will try different combinations of Random Forest hyperparameters.","metadata":{"id":"4YfkSuuwKu5n"}},{"cell_type":"markdown","source":"\n\n#### Most important hyperparameters of Random Forest:\n\nn_estimators = n of trees\n\nmax_depth = max number of levels in each decision tree\n\nbootstrap = method for sampling data points (with or without replacement)","metadata":{"id":"sFaNjd9aKu5n"}},{"cell_type":"markdown","source":"Here, we will choose parameters using GridSearchCV with 5-fold cross validations.\n\nThe scoring is set to *neg_mean_squared_error* to compute the MSE of the model. n_jobs=-1 means that we will use all the processors to boost the running time.\n\nLet's see how well this model performs!","metadata":{"id":"nlzBTdjJ-D0f"}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nn_estimators = [64, 128, 256, 512, 1024]\nmax_depth = np.linspace(1, 32, 16, endpoint=True)\nmin_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\nmin_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)\n\nparam_grid = {\n    \"n_estimators\": n_estimators,\n    \"max_depth\": max_depth,\n    \"bootstrap\": [True, False]\n}\n\nforest = RandomForestRegressor(n_jobs=-1, random_state=3) \n\ngrid_search_forest = GridSearchCV(forest, param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search_forest.fit(X_train, y_train)\ncvres = grid_search_forest.cv_results_\n\ngrid_search_forest.best_estimator_\n\ngrid_best= grid_search_forest.best_estimator_.predict(X_test)\n\nfrom sklearn.metrics import mean_squared_error\n\ngrid_mse = mean_squared_error(y_test, grid_best)\n \n","metadata":{"id":"S4dU4j58Ku5n","outputId":"c1651824-4758-4e9c-8cdc-5aff2771312b","scrolled":true,"execution":{"iopub.status.busy":"2022-06-17T06:36:22.868382Z","iopub.execute_input":"2022-06-17T06:36:22.869139Z","iopub.status.idle":"2022-06-17T06:36:42.499306Z","shell.execute_reply.started":"2022-06-17T06:36:22.869100Z","shell.execute_reply":"2022-06-17T06:36:42.498509Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"print (grid_mse /scores['RFR'])","metadata":{"id":"7p3D5effW_5a","outputId":"2fb8bb51-2edd-4c08-efd7-ab2af128556c","execution":{"iopub.status.busy":"2022-06-17T06:40:01.831941Z","iopub.execute_input":"2022-06-17T06:40:01.832323Z","iopub.status.idle":"2022-06-17T06:40:01.837714Z","shell.execute_reply.started":"2022-06-17T06:40:01.832292Z","shell.execute_reply":"2022-06-17T06:40:01.836705Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"> After conducting hyperpararameter tuning for Random Forest model, we can see 1% improvement in MSE of the prediction prices.","metadata":{"id":"B_CTZ9bzV0E_"}},{"cell_type":"markdown","source":"### *XGBoost Regressor*","metadata":{"id":"iVc78vJBKu5n"}},{"cell_type":"markdown","source":"For this model, we will also choose parameters using GridSearchCV with 5-fold cross validations.","metadata":{"id":"imeSYjzn-uqR"}},{"cell_type":"code","source":"xgb_hyperparameters_grid1 = {\n    \"n_estimators\": [i for i in range (50, 150, 10)],\n    \"learning_rate\": [0.1, 0.2, 0.3],\n    \"max_depth\": [i for i in range (11)],\n    \"bootstrap\" : [True, False]\n}\n    \nmodel_xgb = XGBRegressor(n_jobs = -1)\n\nxgb_grid_search1 = GridSearchCV(\n    model_xgb, xgb_hyperparameters_grid1, cv = 5, scoring = \"neg_mean_squared_error\")\nxgb_grid_search1.fit(X_train, y_train)","metadata":{"id":"GH1ZUIrbKu5o","outputId":"fee46ab7-eded-44a5-e73e-439fcd872204","execution":{"iopub.status.busy":"2022-06-17T06:40:13.903184Z","iopub.execute_input":"2022-06-17T06:40:13.903576Z","iopub.status.idle":"2022-06-17T07:28:41.046593Z","shell.execute_reply.started":"2022-06-17T06:40:13.903544Z","shell.execute_reply":"2022-06-17T07:28:41.045766Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"grid_best= xgb_grid_search1.best_estimator_.predict(X_test)","metadata":{"id":"yjsd6UdrKu5o","execution":{"iopub.status.busy":"2022-06-17T07:28:41.048113Z","iopub.execute_input":"2022-06-17T07:28:41.048514Z","iopub.status.idle":"2022-06-17T07:28:41.070614Z","shell.execute_reply.started":"2022-06-17T07:28:41.048479Z","shell.execute_reply":"2022-06-17T07:28:41.069881Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"xgb_grid_search1.best_params_","metadata":{"id":"JQ4ZKJ40Ku5o","outputId":"795094fb-811b-4b2b-b0a3-758b903e7cb1","execution":{"iopub.status.busy":"2022-06-17T07:28:41.074723Z","iopub.execute_input":"2022-06-17T07:28:41.076656Z","iopub.status.idle":"2022-06-17T07:28:41.082113Z","shell.execute_reply.started":"2022-06-17T07:28:41.076629Z","shell.execute_reply":"2022-06-17T07:28:41.081068Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"grid_mse = mean_squared_error(y_test, grid_best)","metadata":{"id":"jswIkmb9Ku5o","execution":{"iopub.status.busy":"2022-06-17T07:28:41.084324Z","iopub.execute_input":"2022-06-17T07:28:41.085177Z","iopub.status.idle":"2022-06-17T07:28:41.090622Z","shell.execute_reply.started":"2022-06-17T07:28:41.085138Z","shell.execute_reply":"2022-06-17T07:28:41.089765Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"print (grid_mse /scores['XGB'])","metadata":{"id":"IxSpyPLJKu5o","outputId":"93011250-8a57-426e-8d5f-5115cc9bef60","execution":{"iopub.status.busy":"2022-06-17T07:28:41.091801Z","iopub.execute_input":"2022-06-17T07:28:41.092654Z","iopub.status.idle":"2022-06-17T07:28:41.098720Z","shell.execute_reply.started":"2022-06-17T07:28:41.092622Z","shell.execute_reply":"2022-06-17T07:28:41.097933Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"> After conducting hyperpararameter tuning for XGBoost model, we can see 4% improvement in MSE of the prediction prices.","metadata":{"id":"N253Re_5_dQ_"}},{"cell_type":"markdown","source":"# CONCLUSION\nIn this project, we have utilized real historical housing transactions to derive valuable insights into the Melbourne housing market.\n\nAlso, we've trained and conducted hyperparameter tuning and PCA on different models to predict house prices which helps home buyers to make better decisions.\n\nAfter all, we end up with two best models (Random Forest Regressor and XGBoost Regressor) which seem to perform well on this dataset with no sign of overfitting.","metadata":{"id":"9WyVuuW6_uiO"}},{"cell_type":"markdown","source":"# Thank you for your attention!\n---","metadata":{"id":"RrdRqrTCKu5p"}}]}